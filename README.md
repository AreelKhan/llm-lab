# Welcome to LLM-lab!

This is a repo of all my experiments and notes while learning about LLMs.

Some things I intend on doing:

- Follow Andrej Karpathy's [Neural Networks: Zero to Hero guide](https://youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&si=WLr50vjq8ordxLttz). This will help review and learn hands-on the inner-workings of transformers.
- Run LLAMA models locally (I've never actually done this before!).
- Quantize a locally running LLM and benchmark efficiency improvements while monitoring accuracy and other performance metrics.
- Implement FlashAttention1 and 2 by hand (in Cuda maybe?)
- Implement some naive version of speculative decoding.
- Implement FSDP by hand (no idea how!?)
- Experiment with knowledge distillation.
- Many other things that will come up!